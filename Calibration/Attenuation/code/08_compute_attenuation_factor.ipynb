{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot surface effect; and compute attenuation factor $B(\\omega)$\n",
    "2021.10.05 Kurama Okubo\n",
    "\n",
    "Summary plots for the directionality of sensors including surface effect. This note book is modifed from `SensorCoupling_BallDrop/code/08_plot_surfaceeffect_result_Case2.ipynb`.\n",
    "\n",
    "- 2022.1.5 plot schematic\n",
    "- 2022.11.25 update for new ball-drop test\n",
    "- 2024.1.24 update for master plot; plotting Case 2 with the threshold of multiple P arrival with large incident angle.\n",
    "- 2024.7.17 update to compute attenuation factor B(Ï‰).\n",
    "- 2024.7.18 update to apply smoothing on the spectra.\n",
    "- 2025.3.16 update for master plot. Clean up the notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import obspy\n",
    "from obspy import read, Stream, Trace\n",
    "from scipy import signal\n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "%matplotlib inline\n",
    "import glob\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import mpmath as mp\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "\n",
    "from matplotlib import gridspec\n",
    "from obspy.signal.cross_correlation import correlate, xcorr_max\n",
    "\n",
    "from scipy import interpolate\n",
    "from scipy.optimize import curve_fit  \n",
    "import matplotlib as mpl\n",
    "import pickle\n",
    "\n",
    "plt.rcParams[\"font.family\"] = 'Arial'\n",
    "# plt.rcParams[\"font.sans-serif\"] = \"DejaVu Sans, Arial, Helvetica, Lucida Grande, Verdana, Geneva, Lucid, Avant Garde, sans-serif\"\n",
    "plt.rcParams[\"font.size\"] = 9\n",
    "plt.rcParams[\"xtick.direction\"] = \"in\"\n",
    "plt.rcParams[\"xtick.major.size\"] = 4.75\n",
    "plt.rcParams[\"xtick.major.width\"] = 0.75\n",
    "plt.rcParams[\"xtick.minor.size\"] = 3\n",
    "plt.rcParams[\"xtick.minor.width\"] = 0.4\n",
    "plt.rcParams[\"xtick.minor.visible\"] = True\n",
    "\n",
    "plt.rcParams[\"ytick.direction\"] = \"in\"\n",
    "plt.rcParams[\"ytick.major.size\"] = 4.75\n",
    "plt.rcParams[\"ytick.major.width\"] = 0.75\n",
    "plt.rcParams[\"ytick.minor.size\"] = 3\n",
    "plt.rcParams[\"ytick.minor.width\"] = 0.4\n",
    "plt.rcParams[\"ytick.minor.visible\"] = True\n",
    "\n",
    "plt.rcParams[\"savefig.transparent\"] = True\n",
    "plt.rcParams['axes.linewidth'] = 0.75\n",
    "\n",
    "from obspy.core.utcdatetime import UTCDateTime  \n",
    "os.environ['TZ'] = 'GMT' # change time zone to avoid confusion in unix_tvec conversion\n",
    "UTCDateTime.DEFAULT_PRECISION = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory for picked waveforms\n",
    "rootdir = \"../../SensorCoupling_BallDrop/code/\"\n",
    "# bdwaveform_datadir = rootdir+\"../data/DATA_greencomparison\"\n",
    "bdwaveform_datadir = \"../data/DATA_greencomparison\" # use the longer pretrigger for observation noise\n",
    "\n",
    "# channel table\n",
    "channel_finame = '../../../Others/AEchanneltable/AEsensorlocation_onFB03_table.csv'\n",
    "\n",
    "# source location\n",
    "source_finame = rootdir+'../data/balldrop_events_isocoord.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figdir = \"../figure/debug_08_surfaceeffect_casestudy/Case2_schematic/\"\n",
    "if not os.path.exists(figdir):\n",
    "    os.makedirs(figdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root=rootdir+\"../data/DATA_surfaceeffect_Aij_Case2/\" # read the coupling coefficient from the original datasheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read PAZ scaling factor as the gain k_{PAZ}\n",
    "# parameters for instrumental response removal\n",
    "PAZ_case = \"fronttop\"\n",
    "# D = sio.loadmat(\"../data/AE_resp_dataandcoef_{}.mat\".format(PAZ_case));\n",
    "D = sio.loadmat(rootdir+\"../../AEsensor_Calibration/AEsensor_Calibration_ARX/data/AE_resp_dataandcoef_{}.mat\".format(PAZ_case));\n",
    "poles = np.squeeze(D[\"p\"])\n",
    "zeros = np.squeeze(D[\"z\"])\n",
    "scale_fac = np.squeeze(D[\"k\"])\n",
    "u_normfact = np.squeeze(D[\"u_normfact\"]) # normalized factor during the estimation of poles and zeros. see the following cell of note.\n",
    "\n",
    "preamp = 40 #[dB]\n",
    "\n",
    "k_PAZ = scale_fac/10**(preamp/20)# scale_fac/u_normfact/10**(preamp/20) # we already scaled with u_normfact\n",
    "\n",
    "print(f\"k_PAZ = {k_PAZ:.2f} [V/(m/s)]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_fac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root+\"directionality_indextable.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_fac/u_normfact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the sensor coupling factor results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if vectorization is done in a same order between model 1&2 and 3&4 in matlab code.\n",
    "df_index12 = pd.read_csv(root+\"directionality_indextable.csv\", header=None, names=[\"1\", \"2\"]) \n",
    "df_index34 = pd.read_csv(root+\"directionality_indextable_mix.csv\", header=None, names=[\"1\", \"2\"]) \n",
    "assert(df_index12 == df_index34).all().all()\n",
    "df_indextable = df_index12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_disttheta12 = pd.read_csv(root+\"directionality_distandtheta.csv\", header=None, names=[\"1\", \"2\", \"3\", \"4\", \"5\"]) \n",
    "df_disttheta34 = pd.read_csv(root+\"directionality_distandtheta_mix.csv\", header=None, names=[\"1\", \"2\", \"3\", \"4\", \"5\"]) \n",
    "assert(df_disttheta12 == df_disttheta34).all().all()\n",
    "df_disttheta = df_disttheta12\n",
    "df_disttheta.columns=[\"r\", \"l\", \"lr\", \"theta\", \"alpha\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_disttheta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_caseid = pd.read_csv(root+\"directionality_caseid.csv\", header=None, names=[\"caseid\"]) \n",
    "df_caseid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read inversion results\n",
    "df_Si12 = pd.read_csv(root+\"directionality_Si.csv\", header=None, names=[\"model1\", \"model2\"]) \n",
    "df_Si34 = pd.read_csv(root+\"directionality_Si_mix.csv\", header=None, names=[\"model3\", \"model4\"]) \n",
    "df_Si = df_Si12.join(df_Si34)\n",
    "df_Si.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Tj12 = pd.read_csv(root+\"directionality_Tj.csv\", header=None, names=[\"model2\"]) \n",
    "df_Tj34 = pd.read_csv(root+\"directionality_Tj_mix.csv\", header=None, names=[\"model4\"]) \n",
    "df_Tj = df_Tj12.join(df_Tj34)\n",
    "df_Tj.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Akest12 = pd.read_csv(root+\"directionality_Akest.csv\", header=None, names=[\"model1\", \"model2\"]) \n",
    "df_Akest34 = pd.read_csv(root+\"directionality_Akest_mix.csv\", header=None, names=[\"model3\", \"model4\"]) \n",
    "df_Akest = df_Akest12.join(df_Akest34)\n",
    "df_Akest.apply(lambda x: x*1e3) # showing the amplitude in (mm/s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Aijobs = pd.read_csv(root+\"Aijobs.csv\", header=None) \n",
    "Aijobs = df_Aijobs.values\n",
    "Ns, Nb = Aijobs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE** The unit of residual is (m/s) as the Matlab's `lsqlin` and `lsqnonlin` outputs the vector of the residual of $Cx - d$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res12 = pd.read_csv(root+\"directionality_residual.csv\", header=None, names=[\"model1\", \"model2\"]) \n",
    "df_res34 = pd.read_csv(root+\"directionality_residual_mix.csv\", header=None, names=[\"model3\", \"model4\"]) \n",
    "df_res = df_res12.join(df_res34)\n",
    "df_res.head()\n",
    "np.sqrt(np.square(df_res).sum(axis=0)) * 1e3 # [mm/s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimized source duration associated with the aperture effect term\n",
    "df_TR = pd.read_csv(root+\"directionality_TR_mix.csv\", header=None, names=[\"model3\",\"model4\"]) \n",
    "df_TR.apply(lambda x: x*1e6).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_AIC12 = pd.read_csv(root+\"directionality_AIC.csv\", header=None, names=[\"model1\", \"model2\", \"model1c\", \"model2c\"]) \n",
    "df_AIC34 = pd.read_csv(root+\"directionality_AIC_mix.csv\", header=None, names=[\"model3\", \"model4\", \"model3c\", \"model4c\"]) \n",
    "df_AIC = df_AIC12.join(df_AIC34)\n",
    "df_AIC.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Aijsyn = pd.read_csv(root+\"Aijsyn.csv\", header=None) \n",
    "df_Aijobs = pd.read_csv(root+\"Aijobs.csv\", header=None) \n",
    "\n",
    "Aijsyn = df_Aijsyn.values\n",
    "Aijobs = df_Aijobs.values\n",
    "Aksyn = []\n",
    "Akobs = []\n",
    "for _, row in df_indextable.iterrows():\n",
    "    i = row['1']-1\n",
    "    j = row['2']-1\n",
    "    Aksyn.append(Aijsyn[i, j])\n",
    "    Akobs.append(Aijobs[i, j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Channel Index\n",
    "df_array = pd.read_csv(channel_finame)\n",
    "\n",
    "channel_loc={}\n",
    "\n",
    "for i in range(len(df_array)):\n",
    "    stnm = df_array.iloc[i].Instrument_Label\n",
    "    xtemp = df_array.iloc[i].North.astype('float')\n",
    "    ytemp = df_array.iloc[i].East.astype('float')\n",
    "    ztemp = df_array.iloc[i].Down.astype('float')\n",
    "    channel_loc[stnm] = [xtemp, ytemp, ztemp]\n",
    "    \n",
    "AEsensors = list(channel_loc.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read source location\n",
    "# NOTE: Aij is assembled with j sorted from BD-0223 to BD-2973.\n",
    "sourceloc_finame = rootdir+'../data/balldrop_locations.csv'#'../data/balldrop_events_isocoord.csv'\n",
    "df_location = pd.read_csv(sourceloc_finame, index_col=0)\n",
    "df_location.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute constants\n",
    "Nd = len(df_indextable)\n",
    "Ns = df_indextable['1'].max()\n",
    "Nb = df_indextable['2'].max()\n",
    "Nd, Ns, Nb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the aperture effect factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sensitivity of surface effect\n",
    "def incidentangle_scalingfactor_analytic(v, theta, TR, R):\n",
    "    if theta==0:\n",
    "        return 1.0\n",
    "    else:\n",
    "        va = v/np.sin(theta)\n",
    "        J1 = mp.besselj(1, (2*np.pi*R)/(va*TR))\n",
    "        return  ((va * TR)/(np.pi*R)) * J1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = 6200\n",
    "R = 6.35e-3 # 8.0e-3\n",
    "TR_model4 = df_TR[\"model4\"].values[0]\n",
    "theta_vec=np.linspace(0, 90, 101)\n",
    "\n",
    "kvec = np.array([incidentangle_scalingfactor_analytic(v, np.deg2rad(x), TR_model4, R) for x in theta_vec])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define freqband\n",
    "# freqmin = 0.01e6 \n",
    "# freqmax = 0.6e6 #1.0e6\n",
    "# freqband = '{:.0f}-{:.0f}'.format(freqmin/1e3, freqmax/1e3)\n",
    "vmean = 6200\n",
    "pwin_len_pre = 5e-3 #[ms]: buffer of p window length to estimate gain\n",
    "pwin_len = 15e-3 #[ms]: p window length to estimate gain\n",
    "dist_p_threshold = 200 #300 #[mm]: use tpick within this threshold, otherwise use theoretical p arrival \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set case id pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse beteeen index i and j and caseid\n",
    "cij ={}\n",
    "\n",
    "for index, row in df_indextable.iterrows():\n",
    "    i, j = row\n",
    "    stcaseid = df_caseid.iloc[index].values[0]\n",
    "    stnm, caseid = stcaseid.split('__')\n",
    "    if not stcaseid in cij:\n",
    "        # append as new stcase\n",
    "        cij[stcaseid] = [i, j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_indextable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cij)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kk = 100\n",
    "df_disttheta[\"theta\"][kk]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute the spectrum of observation and synthetic P waveform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of processing\n",
    "We process an example of the pair to extract the meta data of the waveforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stnm_prelim = \"OL23\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finame = os.path.join(bdwaveform_datadir, \"{}_bdwaveform_longpretrig.pickle\".format(stnm_prelim)) \n",
    "st = read(finame, format=\"PICKLE\")\n",
    "\n",
    "# prefilitering traces\n",
    "# demean\n",
    "st.detrend(type='demean')\n",
    "#     # apply bandpass\n",
    "#     st.filter(\"bandpass\", freqmin=freqmin, freqmax=freqmax, corners=4, zerophase=True)    \n",
    "# apply taper\n",
    "st.taper(max_percentage=0.001)   \n",
    "\n",
    "# Select observation and synthetic\n",
    "st_obs = st.select(channel='OY')\n",
    "st_syn = st.select(channel='SY')\n",
    "\n",
    "# Extract the traces used in the Si inversion\n",
    "for i, tr_obs in enumerate(st_obs):\n",
    "    if tr_obs.stats.dataindex not in np.squeeze(df_caseid.values):\n",
    "        print(tr_obs.stats.dataindex)\n",
    "        st_obs.remove(tr_obs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select observation trace\n",
    "tr_obs = st_obs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read amplitude correction factors\n",
    "\n",
    "dataindex = tr_obs.stats.dataindex\n",
    "tr_syn = list(filter(lambda tr: tr.stats.dataindex == dataindex, st_syn))[0]\n",
    "\n",
    "dist = tr_obs.stats.dist #[mm]\n",
    "\n",
    "if dist <= dist_p_threshold:\n",
    "    p_theoretical = tr_obs.stats.tpick\n",
    "else:\n",
    "    p_theoretical = dist/vmean\n",
    "\n",
    "tvec = np.array(range(0, tr_syn.stats.npts))*tr_syn.stats.delta*1e3 - tr_obs.stats.pretrigger #[ms]\n",
    "\n",
    "#---compute correction coefficient from Si, Tj and alpha(theta)---#\n",
    "amp_correction = np.zeros(4) # 4 models\n",
    "ii, jj = [cij[dataindex][0]-1, cij[dataindex][1]-1]\n",
    "kk = df_caseid.index[df_caseid.caseid == dataindex][0]\n",
    "\n",
    "Asyn = Aksyn[kk]\n",
    "\n",
    "# model 1: gain model\n",
    "Si1 = df_Si[\"model1\"][ii]\n",
    "amp_correction[0] = Si1\n",
    "\n",
    "# model 3: directionality-dominant\n",
    "Si3 = df_Si[\"model3\"][ii]\n",
    "theta3 = df_disttheta[\"theta\"][kk]\n",
    "#             alpha3 = model_bell(theta3, 1.0, a3, b3) # bell-shape \n",
    "#             amp_correction[1] = Si3*alpha3\n",
    "TR3 = df_TR[\"model3\"][0]\n",
    "k3 = incidentangle_scalingfactor_analytic(v, np.deg2rad(theta3), TR3, R)\n",
    "amp_correction[1] = Si3*k3\n",
    "#model 2: source impact-dominant\n",
    "Si2 = df_Si[\"model2\"][ii]\n",
    "Tj2 = df_Tj[\"model2\"][jj]\n",
    "amp_correction[2] = Si2*Tj2\n",
    "\n",
    "# model 4: mix model\n",
    "Si4 = df_Si[\"model4\"][ii]\n",
    "Tj4 = df_Tj[\"model4\"][jj]\n",
    "theta4 = df_disttheta[\"theta\"][kk]\n",
    "#             alpha4 = model_bell(theta4, 1.0, a4, b4) # bell-shape \n",
    "TR4 = df_TR[\"model4\"][0]\n",
    "k4 = incidentangle_scalingfactor_analytic(v, np.deg2rad(theta4), TR4, R)\n",
    "\n",
    "amp_correction[3] = Si4*Tj4*k4\n",
    "\n",
    "print(amp_correction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st0 = tr_obs.stats.starttime\n",
    "pt = timedelta(milliseconds = tr_obs.stats.pretrigger)\n",
    "\n",
    "dist = tr_obs.stats.dist #[mm]\n",
    "\n",
    "if dist <= dist_p_threshold:\n",
    "    p_theoretical = tr_obs.stats.tpick\n",
    "else:\n",
    "    p_theoretical = dist/vmean\n",
    "\n",
    "starttime = st0+pt+(p_theoretical-pwin_len_pre)*1e-3\n",
    "endtime = st0+pt+(p_theoretical-pwin_len_pre+pwin_len)*1e-3\n",
    "\n",
    "tr_obs_trim = tr_obs.copy() # avoid error in multiple trim\n",
    "tr_obs_trim.trim(starttime, endtime, pad=True, fill_value=0.0)\n",
    "tr_obs_trim.data /= amp_correction[-1]\n",
    "\n",
    "# trim synthetic data\n",
    "tr_syn_trim = tr_syn.copy() # avoid error in multiple trim\n",
    "tr_syn_trim.trim(starttime, endtime, pad=True, fill_value=0.0)\n",
    "    \n",
    "# trim the observation noise\n",
    "noise_Pbackward = 2e-6\n",
    "noise_winlen = 15e-6 # [s]\n",
    "if p_theoretical*1e-3-noise_Pbackward < noise_winlen:\n",
    "    print(\"P_theoretical is smaller than noise winlen\")\n",
    "    \n",
    "tr_obs_noise = tr_obs.copy() # avoid error in multiple trim\n",
    "\n",
    "noise_end_time = st0+pt+p_theoretical*1e-3-noise_Pbackward\n",
    "noise_start_time = noise_end_time - noise_winlen\n",
    "\n",
    "tr_obs_noise.trim(noise_start_time, noise_end_time, pad=True, fill_value=0.0) # trim before the p wave window\n",
    "tr_obs_noise.data /= amp_correction[-1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_obs_trim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the trimming\n",
    "plt.plot(tr_obs.times()*1e6, tr_obs.data/amp_correction[-1], \"k-\", label=\"raw\")\n",
    "plt.plot((pt.microseconds+(p_theoretical-pwin_len_pre)*1e3) + tr_obs_trim.times()*1e6, tr_obs_trim.data, \"r--\", label=\"trimmed\")\n",
    "plt.xlim([60, 90])\n",
    "plt.ylim([-5e-4, 5e-4])\n",
    "plt.legend(loc=0)\n",
    "plt.title(f\"{tr_obs.stats.dataindex}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preprocessing\n",
    "tr_obs_trim.detrend(type=\"linear\").taper(max_percentage = 0.05).detrend(type=\"demean\")\n",
    "tr_syn_trim.detrend(type=\"linear\").taper(max_percentage = 0.05).detrend(type=\"demean\")\n",
    "tr_obs_noise.detrend(type=\"linear\").taper(max_percentage = 0.05).detrend(type=\"demean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute maximum cross-correlation function\n",
    "taper_percent = 0.5\n",
    "max_lag_shift = 30\n",
    "\n",
    "cc = correlate(tr_obs_trim.taper(max_percentage = taper_percent, type='hann'), tr_syn_trim.taper(max_percentage = taper_percent, type='hann'), max_lag_shift, demean=True, normalize='naive') # the order is 1. obs and 2. syn\n",
    "N_shift, _ = xcorr_max(cc,  abs_max=False)\n",
    "N_shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the max cc time shift works\n",
    "plt.plot(tr_obs_trim.times(), tr_obs_trim.data, \"k-\", label=\"observation\")\n",
    "plt.plot(tr_obs_trim.times(), tr_syn_trim.data, \"b--\", label=\"original\")\n",
    "plt.plot(tr_obs_trim.times(), np.roll(tr_syn_trim.data, N_shift), \"r-\", label=\"time shifted\")\n",
    "plt.plot(tr_obs_noise.times(), tr_obs_noise.data, \"-\", c=\"gray\", label=\"noise data\")\n",
    "# Compute RMSE\n",
    "RMSE = np.sqrt(np.mean(np.square(tr_obs_trim.data - np.roll(tr_syn_trim.data, N_shift))))\n",
    "plt.title(f\"{tr_obs_trim.stats.dataindex} RMSE={RMSE:.2g}\")\n",
    "plt.legend(loc=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply smoothing on the spectra\n",
    "\n",
    "We follow Clarke et al. (2011) to apply the smoothing on the spectra. The function is implemented in MSNoise (https://github.com/ROBelgium/MSNoise/blob/5b12a782b67e9138898259f1729e1d805a6d7eb0/msnoise/s05compute_mwcs2.py#L252C20-L266C69).\n",
    "\n",
    "We convolve the hanning window on the spectra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NFFT = 2**8\n",
    "print(len(tr_obs_trim.times()), len(tr_obs_noise.times()), NFFT)\n",
    "smoothing_half_win = 3\n",
    "hanningwindow = signal.windows.hann(2*smoothing_half_win+1).astype('complex')\n",
    "hanningwindow /= smoothing_half_win #len(hanningwindow) # divide by half win so that the summation of window weights is unity\n",
    "print(np.sum(hanningwindow))\n",
    "\n",
    "# smoothing half width in frequency\n",
    "\n",
    "# Compute frequency vector for FFT\n",
    "Y_obs_freq = np.fft.rfftfreq(NFFT, d=tr_obs_trim.stats.delta)\n",
    "\n",
    "dfreq= Y_obs_freq[1] - Y_obs_freq[0]\n",
    "smooth_half_win_freq = smoothing_half_win * dfreq\n",
    "\n",
    "print(f\"smoothing half win:{smooth_half_win_freq/1e3}kHz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(hanningwindow.real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(hanningwindow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process all the pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figdir_Bomega = \"../figure/p08_Bomega/\"\n",
    "if not os.path.exists(figdir_Bomega):\n",
    "    os.makedirs(figdir_Bomega)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2**8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ampfactor_comparison = np.zeros((0, 4))\n",
    "trcount = 0\n",
    "theta_evalerr = np.zeros(Nd)\n",
    "err_evalerr = np.zeros(Nd)\n",
    "\n",
    "pwin_len_margin = 3e-6 # to avoid the discontinuity due to the roll time shift\n",
    "\n",
    "# NFFT = 2**8 # defined above\n",
    "noise_Pbackward = 5e-6\n",
    "noise_winlen = 15e-6 # [s]\n",
    "# smoothing_half_win = 3  # defined above\n",
    "\n",
    "Bomega_all = dict()\n",
    "taper_percent = 0.5 #0.1\n",
    "max_lag_shift = 30\n",
    "\n",
    "# centralize the STF to avoid asymmetric taper\n",
    "centralize_loc = (pwin_len*1e-3) / 2 + pwin_len_margin\n",
    "\n",
    "hanningwindow = signal.windows.hann(2*smoothing_half_win+1).astype('complex')\n",
    "hanningwindow /= smoothing_half_win #len(hanningwindow) # divide by half win so that the summation of window weights is unity\n",
    "print(np.sum(hanningwindow))\n",
    "\n",
    "# smoothing half width in frequency\n",
    "\n",
    "dfreq= Y_obs_freq[1] - Y_obs_freq[0]\n",
    "smooth_half_win_freq = smoothing_half_win * dfreq\n",
    "print(f\"smoothing half win:{smooth_half_win_freq/1e3}kHz\")\n",
    "\n",
    "for stnm in tqdm(AEsensors):\n",
    "# for stnm in tqdm(AEsensors[19:20]):\n",
    "    # stnm = AEsensors[11]\n",
    "    finame = os.path.join(bdwaveform_datadir, \"{}_bdwaveform_longpretrig.pickle\".format(stnm)) \n",
    "    st = read(finame, format=\"PICKLE\")\n",
    "\n",
    "    # prefilitering traces\n",
    "    # demean\n",
    "#     st.detrend(type='demean')\n",
    "#     # apply bandpass\n",
    "#     st.filter(\"bandpass\", freqmin=freqmin, freqmax=freqmax, corners=4, zerophase=True)     # NO FILTER for the analysis of attenuation\n",
    "    # apply taper\n",
    "#     st.taper(max_percentage=0.001)   \n",
    "\n",
    "    # Select observation and synthetic\n",
    "    st_obs = st.select(channel='OY')\n",
    "    st_syn = st.select(channel='SY')\n",
    "    \n",
    "    # Extract the traces used in the Si inversion\n",
    "    for i, tr_obs in enumerate(st_obs):\n",
    "        if tr_obs.stats.dataindex not in np.squeeze(df_caseid.values):\n",
    "            print(tr_obs.stats.dataindex)\n",
    "            st_obs.remove(tr_obs)\n",
    "    \n",
    "    for i, tr_obs in enumerate(st_obs):\n",
    "\n",
    "            dataindex = tr_obs.stats.dataindex\n",
    "            tr_syn = list(filter(lambda tr: tr.stats.dataindex == dataindex, st_syn))[0]\n",
    "\n",
    "            dist = tr_obs.stats.dist #[mm]\n",
    "\n",
    "            if dist <= dist_p_threshold:\n",
    "                p_theoretical = tr_obs.stats.tpick\n",
    "            else:\n",
    "                p_theoretical = dist/vmean\n",
    "\n",
    "            tvec = np.array(range(0, tr_syn.stats.npts))*tr_syn.stats.delta*1e3 - tr_obs.stats.pretrigger #[ms]\n",
    "\n",
    "            #---compute correction coefficient from Si, Tj and alpha(theta)---#\n",
    "            amp_correction = np.zeros(4) # 4 models\n",
    "            ii, jj = [cij[dataindex][0]-1, cij[dataindex][1]-1]\n",
    "            kk = df_caseid.index[df_caseid.caseid == dataindex][0]\n",
    "\n",
    "            Asyn = Aksyn[kk]\n",
    "\n",
    "            # model 1: gain model\n",
    "            Si1 = df_Si[\"model1\"][ii]\n",
    "            amp_correction[0] = Si1\n",
    "\n",
    "            # model 3: directionality-dominant\n",
    "            Si3 = df_Si[\"model3\"][ii]\n",
    "            theta3 = df_disttheta[\"theta\"][kk]\n",
    "#             alpha3 = model_bell(theta3, 1.0, a3, b3) # bell-shape \n",
    "#             amp_correction[1] = Si3*alpha3\n",
    "            TR3 = df_TR[\"model3\"][0]\n",
    "            k3 = incidentangle_scalingfactor_analytic(v, np.deg2rad(theta3), TR3, R)\n",
    "            amp_correction[1] = Si3*k3\n",
    "            #model 2: source impact-dominant\n",
    "            Si2 = df_Si[\"model2\"][ii]\n",
    "            Tj2 = df_Tj[\"model2\"][jj]\n",
    "            amp_correction[2] = Si2*Tj2\n",
    "            \n",
    "            # model 4: mix model\n",
    "            Si4 = df_Si[\"model4\"][ii]\n",
    "            Tj4 = df_Tj[\"model4\"][jj]\n",
    "            theta4 = df_disttheta[\"theta\"][kk]\n",
    "#             alpha4 = model_bell(theta4, 1.0, a4, b4) # bell-shape \n",
    "            TR4 = df_TR[\"model4\"][0]\n",
    "            k4 = incidentangle_scalingfactor_analytic(v, np.deg2rad(theta4), TR4, R)\n",
    "\n",
    "            amp_correction[3] = Si4*Tj4*k4\n",
    "\n",
    "#             print((Si4, Tj4, k4))\n",
    "            ampfactor_comparison = np.vstack((ampfactor_comparison, [df_disttheta[\"theta\"][kk], \n",
    "                            amp_correction[3]/amp_correction[0],  amp_correction[3]/amp_correction[1], amp_correction[3]/amp_correction[2]]))\n",
    "    \n",
    "            #---------------------------------------------------------------------------------#\n",
    "#             print(dataindex+\":\"+str(amp_correction))\n",
    "#             # plot synthetic waveform\n",
    "#             p0 = axs[i, 0].plot(tvec, 1e3*tr_syn.data, 'r-', label='syn', zorder=2)\n",
    "#             axs[i, 1].plot(tvec, 1e3*tr_syn.data, 'r-', label='syn', zorder=2)\n",
    "\n",
    "#             axs[i, 0].plot(tvec, 1e3*tr_obs.data/amp_correction[-1], \"k-\", lw=1.5, label=xcase[-1]) # use the best model with model 4\n",
    "#             axs[i, 1].plot(tvec, 1e3*tr_obs.data/amp_correction[-1], \"k-\", lw=1.5, label=xcase[-1])\n",
    "\n",
    "            #---------------------------------------------------------------------------------#\n",
    "            #===Compute B(omega)===#\n",
    "            #---------------------------------------------------------------------------------#\n",
    "\n",
    "            st0 = tr_obs.stats.starttime\n",
    "            pt = timedelta(milliseconds = tr_obs.stats.pretrigger)\n",
    "\n",
    "            dist = tr_obs.stats.dist #[mm]\n",
    "\n",
    "            if dist <= dist_p_threshold:\n",
    "                p_theoretical = tr_obs.stats.tpick\n",
    "            else:\n",
    "                p_theoretical = dist/vmean\n",
    "\n",
    "            # trim Pwave\n",
    "            starttime = st0+pt+(p_theoretical-pwin_len_pre)*1e-3 - pwin_len_margin\n",
    "            endtime = st0+pt+(p_theoretical-pwin_len_pre+pwin_len)*1e-3 + pwin_len_margin\n",
    "\n",
    "            tr_obs_trim = tr_obs.copy() # avoid error in multiple trim\n",
    "            tr_obs_trim.trim(starttime, endtime, pad=True, fill_value=0.0)\n",
    "            tr_obs_trim.data /= amp_correction[-1]\n",
    "\n",
    "            # trim synthetic data\n",
    "            tr_syn_trim = tr_syn.copy() # avoid error in multiple trim\n",
    "            tr_syn_trim.trim(starttime, endtime, pad=True, fill_value=0.0)\n",
    "\n",
    "            # trim the observation noise\n",
    "\n",
    "            print(f\"noise margin: {((tr_obs.stats.pretrigger+p_theoretical)*1e-3-noise_Pbackward)*1e6}Î¼s\")\n",
    "            if ((tr_obs.stats.pretrigger+p_theoretical)*1e-3-noise_Pbackward)*1e6 < noise_winlen:\n",
    "                print(\"P_theoretical is smaller than noise winlen\")\n",
    "\n",
    "            tr_obs_noise = tr_obs.copy() # avoid error in multiple trim\n",
    "\n",
    "            noise_end_time = st0+pt+p_theoretical*1e-3-noise_Pbackward\n",
    "            noise_start_time = noise_end_time - noise_winlen # synchronize the window length with the P wave pulse\n",
    "\n",
    "            tr_obs_noise.trim(noise_start_time, noise_end_time, pad=True, fill_value=0.0) # trim before the p wave window\n",
    "            tr_obs_noise.data /= amp_correction[-1]\n",
    "\n",
    "            # preprocessing the traces\n",
    "#             tr_obs_trim.detrend(type=\"demean\").detrend(type=\"linear\").taper(max_percentage = 0.05).detrend(type=\"demean\")\n",
    "#             tr_syn_trim.detrend(type=\"demean\").detrend(type=\"linear\").taper(max_percentage = 0.05).detrend(type=\"demean\")\n",
    "#             tr_obs_noise.detrend(type=\"demean\").detrend(type=\"linear\").taper(max_percentage = 0.05).detrend(type=\"demean\")\n",
    "            \n",
    "#             tr_obs_trim.detrend(type=\"demean\").detrend(type=\"linear\").taper(max_percentage = 0.05).detrend(type=\"demean\").detrend(type=\"linear\")\n",
    "#             tr_syn_trim.detrend(type=\"demean\").detrend(type=\"linear\").taper(max_percentage = 0.05).detrend(type=\"demean\").detrend(type=\"linear\")\n",
    "#             tr_obs_noise.detrend(type=\"demean\").detrend(type=\"linear\").taper(max_percentage = 0.05).detrend(type=\"demean\").detrend(type=\"linear\")\n",
    "            \n",
    "            # detrend by the first 5 microsec\n",
    "            noise_npts = int(5e-6*tr_obs_trim.stats.sampling_rate)\n",
    "            for tr in [tr_obs_trim, tr_syn_trim, ]:\n",
    "                kp = np.polyfit(tr.times()[:noise_npts], tr.data[:noise_npts], 1)\n",
    "                tr.data -= np.polyval(kp, tr.times())\n",
    "#                 tr.taper(max_percentage = taper_percent, type='hann') # apply taper after time shifted\n",
    "            \n",
    "            # for noise, detrend by the entire signal\n",
    "            for tr in [tr_obs_noise]:\n",
    "                kp = np.polyfit(tr.times()[:], tr.data[:], 1)\n",
    "                tr.data -= np.polyval(kp, tr.times())\n",
    "                tr.taper(max_percentage = taper_percent, type='hann')\n",
    "                      \n",
    "            #---Update: compute RMSE at max cc--#\n",
    "            # 1. shift the trace at maximum correlation function\n",
    "            cc = correlate(tr_obs_trim.copy().taper(max_percentage = taper_percent).data, tr_syn_trim.copy().taper(max_percentage = taper_percent).data, max_lag_shift, demean=False, normalize='naive') # the order is 1. obs and 2. syn\n",
    "            N_shift, _ = xcorr_max(cc,  abs_max=False)\n",
    "            #-----------------------------------#\n",
    "            tr_syn_trim_shifted = tr_syn_trim.copy()\n",
    "            tr_syn_trim_shifted.data = np.roll(tr_syn_trim.data, N_shift)# assuming the time shift is not large\n",
    "            \n",
    "            # time shift to centralize the trace\n",
    "#             t_syn_center = (tr_syn_trim_shifted.times()[np.argmin(tr_syn_trim_shifted.data)] + tr_syn_trim_shifted.times()[np.argmax(tr_syn_trim_shifted.data)])/2\n",
    "            t_syn_center = (tr_syn_trim_shifted.times()[np.argmax(tr_syn_trim_shifted.data)])\n",
    "            Ncentralize = int((t_syn_center - centralize_loc)/tr_syn_trim_shifted.stats.delta) # The P-wave pulse was centered within a symmetric time window for analysis.\n",
    "            for tr in [tr_obs_trim, tr_syn_trim_shifted, tr_syn_trim]:\n",
    "                tr.data = np.roll(tr.data, -Ncentralize)\n",
    "                \n",
    "            # trim the margin\n",
    "            st0_margin = tr_obs_trim.stats.starttime\n",
    "            starttime_margin = st0_margin + pwin_len_margin\n",
    "            endtime_margin = st0_margin + pwin_len_margin + pwin_len*1e-3\n",
    "\n",
    "            for tr in [tr_obs_trim, tr_syn_trim_shifted, tr_syn_trim]:\n",
    "                tr.trim(starttime_margin, endtime_margin)\n",
    "                # print(f\"data length:{tr.stats.dataindex} {tr.stats.npts}\")\n",
    "            \n",
    "            # apply tapering\n",
    "            tr_obs_trim_raw = tr_obs_trim.copy()\n",
    "            for tr in [tr_obs_trim, tr_syn_trim, tr_syn_trim_shifted, tr_syn_trim]:\n",
    "                tr.taper(max_percentage = taper_percent, type='hann')\n",
    "            \n",
    "            # compute RMSE\n",
    "            RMSE = np.sqrt(np.mean(np.square(tr_obs_trim.data - np.roll(tr_syn_trim_shifted.data, N_shift))))\n",
    "            # compute VR\n",
    "            VRtmp1 = np.linalg.norm(tr_obs_trim.data - tr_syn_trim_shifted.data, axis=0, ord=2) ** 2 \n",
    "            VRtmp2 = np.linalg.norm(tr_obs_trim.data, axis=0, ord=2)**2 # scalled by the observation\n",
    "            VR = 1.0 - VRtmp1/VRtmp2\n",
    "            \n",
    "            # Compute FFT\n",
    "            Y_obs = np.fft.rfft(tr_obs_trim.data, n=NFFT)\n",
    "            Y_syn = np.fft.rfft(tr_syn_trim_shifted.data, n=NFFT)\n",
    "            Y_obs_noise = np.fft.rfft(tr_obs_noise.data, n=NFFT)\n",
    "\n",
    "            Y_obs_freq = np.fft.rfftfreq(NFFT, d=tr_obs_trim.stats.delta)\n",
    "            Y_syn_freq = np.fft.rfftfreq(NFFT, d=tr_syn_trim_shifted.stats.delta)\n",
    "            Y_obs_noise_freq = np.fft.rfftfreq(NFFT, d=tr_obs_noise.stats.delta)\n",
    "\n",
    "            # Apply smoothing\n",
    "            # ref: https://github.com/ROBelgium/MSNoise/blob/master/msnoise/s05compute_mwcs2.py#L92\n",
    "            Y_obs_smoothed = signal.convolve(Y_obs, hanningwindow.real, \"same\")\n",
    "            Y_syn_smoothed = signal.convolve(Y_syn, hanningwindow.real, \"same\")\n",
    "            Y_obs_noise_smoothed = signal.convolve(Y_obs_noise, hanningwindow.real, \"same\")\n",
    "\n",
    "\n",
    "            # compute B(omega)\n",
    "            B_attenu = Y_obs/Y_syn\n",
    "            Battenu_smoothed = Y_obs_smoothed/Y_syn_smoothed\n",
    "\n",
    "#             # Compute maximum cross-correlation function\n",
    "#             signal.correlate(tr_obs_trim.data, tr_syn_trim.data)\n",
    "            \n",
    "            # plot figure for debugging\n",
    "            fig, [ax1, ax2] = plt.subplots(1, 2, figsize=(7.5, 3.58)) #figsize=(9, 4))\n",
    "            \n",
    "            # plot P waveform\n",
    "            ax1.plot(tr_obs_trim.times()*1e6, tr_obs_trim.data*1e3, \"k-\", label=\"observation\")\n",
    "            ax1.plot(tr_obs_trim.times()*1e6, tr_obs_trim_raw.data*1e3, \"k:\", label=\"observation no taper\")\n",
    "            ax1.plot(tr_syn_trim.times()*1e6, tr_syn_trim.data*1e3, \"r:\", label=\"synthetic\")\n",
    "#             ax1.plot(tr_syn_trim.times()*1e6 + N_shift*tr_syn_trim.stats.delta*1e6, tr_syn_trim.data*1e3, \"r--\", label=\"synthetic max. cc\")\n",
    "            ax1.plot(tr_syn_trim_shifted.times()*1e6, tr_syn_trim_shifted.data*1e3, \"r-\", label=\"synthetic shifted\")\n",
    "            ax1.plot(tr_obs_noise.times()*1e6, tr_obs_noise.data*1e3, \"--\", c=\"gray\", label=\"noise\")\n",
    "            ax1.set_xlabel(\"Time [Î¼s]\")\n",
    "            ax1.set_ylabel(\"Velocity [mm/s]\")\n",
    "            ax1.set_title(tr_obs_trim.stats.dataindex)\n",
    "            ax1.set_xlim([0.0, 15])\n",
    "\n",
    "            ylimit_wf_p = np.array([-Asyn, Asyn])*3e3\n",
    "\n",
    "            ax1.set_ylim(ylimit_wf_p)\n",
    "#             ax1.set_ylim([-0.05, 0.05])\n",
    "            \n",
    "            \n",
    "            ax1.text(0.36, 0.05, f\"RMSE={RMSE*1e3:.3f}mm/s, VR={VR:.3f}\\nNshift={N_shift}, Ncenter={Ncentralize}\", transform = ax1.transAxes)\n",
    "            \n",
    "            # plot P wave spectra\n",
    "            amp0 = np.max(np.abs(Y_obs)) # normalize the dB by the maximum of observation\n",
    "            ax2.semilogx(Y_obs_freq/1e6,  20 * np.log10(np.abs(Y_obs)/amp0), \"b.--\", label=\"observation\")\n",
    "            ax2.semilogx(Y_syn_freq/1e6,  20 * np.log10(np.abs(Y_syn)/amp0), \"r.--\", label=\"synthetic\")\n",
    "            ax2.semilogx(Y_obs_noise_freq/1e6,  20 * np.log10(np.abs(Y_obs_noise)/amp0),  \"k.--\", label=\"observation noise\")\n",
    "            ax2.semilogx(Y_obs_freq/1e6,  20 * np.log10(np.abs(B_attenu)), \"g.--\", label=\"B(Ï‰)\")\n",
    "\n",
    "            \n",
    "            amp0_smoothed = np.max(np.abs(Y_obs_smoothed))\n",
    "            ax2.semilogx(Y_obs_freq/1e6,  20 * np.log10(np.abs(Y_obs_smoothed)/amp0_smoothed), \"b.-\", label=\"\")\n",
    "            ax2.semilogx(Y_syn_freq/1e6,  20 * np.log10(np.abs(Y_syn_smoothed)/amp0_smoothed), \"r.-\", label=\"\")\n",
    "            ax2.semilogx(Y_obs_freq/1e6,  20 * np.log10(np.abs(Battenu_smoothed)), \"g.-\", label=\"\")\n",
    "            ax2.semilogx(Y_obs_noise_freq/1e6,  20 * np.log10(np.abs(Y_obs_noise_smoothed)/amp0_smoothed),  \"k.-\", label=\"\")\n",
    "            \n",
    "            ax2.set_xlabel(\"Frequency [MHz]\")\n",
    "            ax2.set_ylabel(\"Amplitude [dB]\")\n",
    "\n",
    "            ax2.set_xlim([0.02, 2])\n",
    "            ax2.set_ylim([-100, 20])\n",
    "            ax2.grid(True, c=np.array([230, 230, 230])/255, lw=0.25, which=\"both\",)\n",
    "            ax2.legend(loc=3, fontsize=9, ncol=2)\n",
    "            ax2.xaxis.set_major_formatter(mticker.FormatStrFormatter('%.1f'))\n",
    "\n",
    "            plt.tight_layout()\n",
    "#             plt.savefig(\"../figure/velocityspectra.png\", dpi=300)\n",
    "#             plt.savefig(\"../figure/velocityspectra.eps\")\n",
    "\n",
    "            plt.savefig(figdir_Bomega+\"/computeBomega_{}.png\".format(dataindex), format=\"png\", dpi=70, bbox_inches=\"tight\")\n",
    "    \n",
    "            # dump the data\n",
    "        \n",
    "            Bomega_all[f\"{dataindex}_dist\"] = tr_obs.stats.dist\n",
    "            Bomega_all[f\"{dataindex}_VR\"] = VR\n",
    "            Bomega_all[f\"{dataindex}_RMSE\"] = RMSE\n",
    "            Bomega_all[f\"{dataindex}_Y_obs\"] = Y_obs\n",
    "            Bomega_all[f\"{dataindex}_Y_syn\"] = Y_syn\n",
    "            Bomega_all[f\"{dataindex}_Y_obs_noise\"] = Y_obs_noise\n",
    "\n",
    "            Bomega_all[f\"{dataindex}_Battenu\"] = B_attenu\n",
    "            Bomega_all[f\"{dataindex}_Battenu_smoothed\"] = Battenu_smoothed\n",
    "             \n",
    "            plt.clf()\n",
    "            plt.close()\n",
    "        \n",
    "        \n",
    "Bomega_all[\"NFFT\"] = NFFT\n",
    "Bomega_all[\"dt\"] = tr_obs_trim.stats.delta\n",
    "Bomega_all[\"noise_Pbackward\"] = noise_Pbackward\n",
    "Bomega_all[\"noise_winlen\"] = NFFT\n",
    "Bomega_all[\"smoothing_half_win\"] = smoothing_half_win\n",
    "Bomega_all[\"smooth_half_win_freq\"] = smooth_half_win_freq\n",
    "Bomega_all[\"taper_percent\"] = taper_percent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/Bomega_all.pickle', mode='wb') as fo:\n",
    "    pickle.dump(Bomega_all, fo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwin_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot figure on the representative source-station pair and attenuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_BDid = '0820' # balldrop id for master plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ampfactor_comparison = np.zeros((0, 4))\n",
    "trcount = 0\n",
    "theta_evalerr = np.zeros(Nd)\n",
    "err_evalerr = np.zeros(Nd)\n",
    "\n",
    "pwin_len_margin = 3e-6 # to avoid the discontinuity due to the roll time shift\n",
    "\n",
    "NFFT = 2**8\n",
    "noise_Pbackward = 5e-6\n",
    "noise_winlen = 15e-6 # [s]\n",
    "smoothing_half_win = 3\n",
    "\n",
    "Bomega_all = dict()\n",
    "taper_percent = 0.5 #0.1\n",
    "max_lag_shift = 30\n",
    "\n",
    "# centralize the STF to avoid asymmetric taper\n",
    "centralize_loc = (pwin_len*1e-3) / 2 + pwin_len_margin\n",
    "\n",
    "hanningwindow = signal.windows.hann(2*smoothing_half_win+1).astype('complex')\n",
    "hanningwindow /= smoothing_half_win #len(hanningwindow) # divide by half win so that the summation of window weights is unity\n",
    "print(np.sum(hanningwindow))\n",
    "\n",
    "# smoothing half width in frequency\n",
    "\n",
    "dfreq= Y_obs_freq[1] - Y_obs_freq[0]\n",
    "smooth_half_win_freq = smoothing_half_win * dfreq\n",
    "print(f\"smoothing half win:{smooth_half_win_freq/1e3}kHz\")\n",
    "\n",
    "# for stnm in tqdm(AEsensors):\n",
    "for stnm in tqdm(AEsensors[19:20]):\n",
    "    # stnm = AEsensors[11]\n",
    "    finame = os.path.join(bdwaveform_datadir, \"{}_bdwaveform_longpretrig.pickle\".format(stnm)) \n",
    "    st = read(finame, format=\"PICKLE\")\n",
    "\n",
    "    # prefilitering traces\n",
    "    # demean\n",
    "#     st.detrend(type='demean')\n",
    "#     # apply bandpass\n",
    "#     st.filter(\"bandpass\", freqmin=freqmin, freqmax=freqmax, corners=4, zerophase=True)     # NO FILTER for the analysis of attenuation\n",
    "    # apply taper\n",
    "#     st.taper(max_percentage=0.001)   \n",
    "\n",
    "    # Select observation and synthetic\n",
    "    st_obs = st.select(channel='OY')\n",
    "    st_syn = st.select(channel='SY')\n",
    "    \n",
    "    # Extract the traces used in the Si inversion\n",
    "    for i, tr_obs in enumerate(st_obs):\n",
    "        if tr_obs.stats.dataindex not in np.squeeze(df_caseid.values):\n",
    "            print(tr_obs.stats.dataindex)\n",
    "            st_obs.remove(tr_obs)\n",
    "    \n",
    "    for i, tr_obs in enumerate(st_obs):\n",
    "        \n",
    "            if tr_obs.stats.location != selected_BDid:\n",
    "                continue\n",
    "                    \n",
    "            dataindex = tr_obs.stats.dataindex\n",
    "            tr_syn = list(filter(lambda tr: tr.stats.dataindex == dataindex, st_syn))[0]\n",
    "\n",
    "            dist = tr_obs.stats.dist #[mm]\n",
    "\n",
    "            if dist <= dist_p_threshold:\n",
    "                p_theoretical = tr_obs.stats.tpick\n",
    "            else:\n",
    "                p_theoretical = dist/vmean\n",
    "\n",
    "            tvec = np.array(range(0, tr_syn.stats.npts))*tr_syn.stats.delta*1e3 - tr_obs.stats.pretrigger #[ms]\n",
    "\n",
    "            #---compute correction coefficient from Si, Tj and alpha(theta)---#\n",
    "            amp_correction = np.zeros(4) # 4 models\n",
    "            print(amp_correction)\n",
    "            ii, jj = [cij[dataindex][0]-1, cij[dataindex][1]-1]\n",
    "            kk = df_caseid.index[df_caseid.caseid == dataindex][0]\n",
    "\n",
    "            Asyn = Aksyn[kk]\n",
    "\n",
    "            # model 1: gain model\n",
    "            Si1 = df_Si[\"model1\"][ii]\n",
    "            amp_correction[0] = Si1\n",
    "\n",
    "            # model 3: directionality-dominant\n",
    "            Si3 = df_Si[\"model3\"][ii]\n",
    "            theta3 = df_disttheta[\"theta\"][kk]\n",
    "#             alpha3 = model_bell(theta3, 1.0, a3, b3) # bell-shape \n",
    "#             amp_correction[1] = Si3*alpha3\n",
    "            TR3 = df_TR[\"model3\"][0]\n",
    "            k3 = incidentangle_scalingfactor_analytic(v, np.deg2rad(theta3), TR3, R)\n",
    "            amp_correction[1] = Si3*k3\n",
    "            #model 2: source impact-dominant\n",
    "            Si2 = df_Si[\"model2\"][ii]\n",
    "            Tj2 = df_Tj[\"model2\"][jj]\n",
    "            amp_correction[2] = Si2*Tj2\n",
    "            \n",
    "            # model 4: mix model\n",
    "            Si4 = df_Si[\"model4\"][ii]\n",
    "            Tj4 = df_Tj[\"model4\"][jj]\n",
    "            theta4 = df_disttheta[\"theta\"][kk]\n",
    "#             alpha4 = model_bell(theta4, 1.0, a4, b4) # bell-shape \n",
    "            TR4 = df_TR[\"model4\"][0]\n",
    "            k4 = incidentangle_scalingfactor_analytic(v, np.deg2rad(theta4), TR4, R)\n",
    "\n",
    "            amp_correction[3] = Si4*Tj4*k4\n",
    "\n",
    "#             print((Si4, Tj4, k4))\n",
    "            ampfactor_comparison = np.vstack((ampfactor_comparison, [df_disttheta[\"theta\"][kk], \n",
    "                            amp_correction[3]/amp_correction[0],  amp_correction[3]/amp_correction[1], amp_correction[3]/amp_correction[2]]))\n",
    "    \n",
    "            #---------------------------------------------------------------------------------#\n",
    "#             print(dataindex+\":\"+str(amp_correction))\n",
    "#             # plot synthetic waveform\n",
    "#             p0 = axs[i, 0].plot(tvec, 1e3*tr_syn.data, 'r-', label='syn', zorder=2)\n",
    "#             axs[i, 1].plot(tvec, 1e3*tr_syn.data, 'r-', label='syn', zorder=2)\n",
    "\n",
    "#             axs[i, 0].plot(tvec, 1e3*tr_obs.data/amp_correction[-1], \"k-\", lw=1.5, label=xcase[-1]) # use the best model with model 4\n",
    "#             axs[i, 1].plot(tvec, 1e3*tr_obs.data/amp_correction[-1], \"k-\", lw=1.5, label=xcase[-1])\n",
    "\n",
    "            #---------------------------------------------------------------------------------#\n",
    "            #===Compute B(omega)===#\n",
    "            #---------------------------------------------------------------------------------#\n",
    "\n",
    "            st0 = tr_obs.stats.starttime\n",
    "            pt = timedelta(milliseconds = tr_obs.stats.pretrigger)\n",
    "\n",
    "            dist = tr_obs.stats.dist #[mm]\n",
    "\n",
    "            if dist <= dist_p_threshold:\n",
    "                p_theoretical = tr_obs.stats.tpick\n",
    "            else:\n",
    "                p_theoretical = dist/vmean\n",
    "\n",
    "            # trim Pwave\n",
    "            starttime = st0+pt+(p_theoretical-pwin_len_pre)*1e-3 - pwin_len_margin\n",
    "            endtime = st0+pt+(p_theoretical-pwin_len_pre+pwin_len)*1e-3 + pwin_len_margin\n",
    "\n",
    "            tr_obs_trim = tr_obs.copy() # avoid error in multiple trim\n",
    "            tr_obs_trim.trim(starttime, endtime, pad=True, fill_value=0.0)\n",
    "            tr_obs_trim.data /= amp_correction[-1]\n",
    "\n",
    "            # trim synthetic data\n",
    "            tr_syn_trim = tr_syn.copy() # avoid error in multiple trim\n",
    "            tr_syn_trim.trim(starttime, endtime, pad=True, fill_value=0.0)\n",
    "\n",
    "            # trim the observation noise\n",
    "\n",
    "            print(f\"noise margin: {((tr_obs.stats.pretrigger+p_theoretical)*1e-3-noise_Pbackward)*1e6}Î¼s\")\n",
    "            if ((tr_obs.stats.pretrigger+p_theoretical)*1e-3-noise_Pbackward)*1e6 < noise_winlen:\n",
    "                print(\"P_theoretical is smaller than noise winlen\")\n",
    "\n",
    "            tr_obs_noise = tr_obs.copy() # avoid error in multiple trim\n",
    "\n",
    "            noise_end_time = st0+pt+p_theoretical*1e-3-noise_Pbackward\n",
    "            noise_start_time = noise_end_time - noise_winlen\n",
    "\n",
    "            tr_obs_noise.trim(noise_start_time, noise_end_time, pad=True, fill_value=0.0) # trim before the p wave window\n",
    "            tr_obs_noise.data /= amp_correction[-1]\n",
    "\n",
    "            # preprocessing the traces\n",
    "#             tr_obs_trim.detrend(type=\"demean\").detrend(type=\"linear\").taper(max_percentage = 0.05).detrend(type=\"demean\")\n",
    "#             tr_syn_trim.detrend(type=\"demean\").detrend(type=\"linear\").taper(max_percentage = 0.05).detrend(type=\"demean\")\n",
    "#             tr_obs_noise.detrend(type=\"demean\").detrend(type=\"linear\").taper(max_percentage = 0.05).detrend(type=\"demean\")\n",
    "            \n",
    "#             tr_obs_trim.detrend(type=\"demean\").detrend(type=\"linear\").taper(max_percentage = 0.05).detrend(type=\"demean\").detrend(type=\"linear\")\n",
    "#             tr_syn_trim.detrend(type=\"demean\").detrend(type=\"linear\").taper(max_percentage = 0.05).detrend(type=\"demean\").detrend(type=\"linear\")\n",
    "#             tr_obs_noise.detrend(type=\"demean\").detrend(type=\"linear\").taper(max_percentage = 0.05).detrend(type=\"demean\").detrend(type=\"linear\")\n",
    "            \n",
    "            # remove offset and detrend by the first 5 microsec\n",
    "            noise_npts = int(5e-6*tr_obs_trim.stats.sampling_rate)\n",
    "            for tr in [tr_obs_trim, tr_syn_trim, ]:\n",
    "                kp = np.polyfit(tr.times()[:noise_npts], tr.data[:noise_npts], 1)\n",
    "                tr.data -= np.polyval(kp, tr.times())\n",
    "#                 tr.taper(max_percentage = taper_percent, type='hann') # apply taper after time shifted\n",
    "            \n",
    "            # for noise, detrend by the entire signal\n",
    "            for tr in [tr_obs_noise]:\n",
    "                kp = np.polyfit(tr.times()[:], tr.data[:], 1)\n",
    "                tr.data -= np.polyval(kp, tr.times())\n",
    "                tr.taper(max_percentage = taper_percent, type='hann')\n",
    "                      \n",
    "            #---Update: compute RMSE at max cc--#\n",
    "            # 1. shift the trace at maximum correlation function\n",
    "            cc = correlate(tr_obs_trim.copy().taper(max_percentage = taper_percent).data, tr_syn_trim.copy().taper(max_percentage = taper_percent).data, max_lag_shift, demean=False, normalize='naive') # the order is 1. obs and 2. syn\n",
    "            N_shift, _ = xcorr_max(cc,  abs_max=False)\n",
    "            #-----------------------------------#\n",
    "            tr_syn_trim_shifted = tr_syn_trim.copy()\n",
    "            tr_syn_trim_shifted.data = np.roll(tr_syn_trim.data, N_shift)# the discontinuity is trimmed by the margin\n",
    "            \n",
    "            # time shift to centralize the trace\n",
    "#             t_syn_center = (tr_syn_trim_shifted.times()[np.argmin(tr_syn_trim_shifted.data)] + tr_syn_trim_shifted.times()[np.argmax(tr_syn_trim_shifted.data)])/2\n",
    "            t_syn_center = (tr_syn_trim_shifted.times()[np.argmax(tr_syn_trim_shifted.data)])\n",
    "            Ncentralize = int((t_syn_center - centralize_loc)/tr_syn_trim_shifted.stats.delta)\n",
    "            for tr in [tr_obs_trim, tr_syn_trim_shifted, ]:\n",
    "                tr.data = np.roll(tr.data, -Ncentralize)\n",
    "            \n",
    "            print(f\"Ncentralize for {tr_obs_trim.stats.dataindex}: {Ncentralize}\")\n",
    "            \n",
    "            # trim the margin\n",
    "            st0_margin = tr_obs_trim.stats.starttime\n",
    "            starttime_margin = st0_margin + pwin_len_margin\n",
    "            endtime_margin = st0_margin + pwin_len_margin + pwin_len*1e-3\n",
    "\n",
    "            for tr in [tr_obs_trim, tr_syn_trim_shifted, tr_syn_trim]:\n",
    "                tr.trim(starttime_margin, endtime_margin)\n",
    "                print(f\"data length:{tr.stats.dataindex} {tr.stats.npts}\")\n",
    "            \n",
    "            # apply tapering\n",
    "            for tr in [tr_obs_trim, tr_syn_trim_shifted, tr_syn_trim]:\n",
    "                tr.taper(max_percentage = taper_percent, type='hann')\n",
    "            \n",
    "            # compute RMSE\n",
    "            RMSE = np.sqrt(np.mean(np.square(tr_obs_trim.data - np.roll(tr_syn_trim_shifted.data, N_shift))))\n",
    "            # compute VR\n",
    "            VRtmp1 = np.linalg.norm(tr_obs_trim.data - tr_syn_trim_shifted.data, axis=0, ord=2) ** 2 \n",
    "            VRtmp2 = np.linalg.norm(tr_obs_trim.data, axis=0, ord=2)**2 # scalled by the observation\n",
    "            VR = 1.0 - VRtmp1/VRtmp2\n",
    "            \n",
    "            # Compute FFT\n",
    "            Y_obs = np.fft.rfft(tr_obs_trim.data, n=NFFT)\n",
    "            Y_syn = np.fft.rfft(tr_syn_trim_shifted.data, n=NFFT)\n",
    "            Y_obs_noise = np.fft.rfft(tr_obs_noise.data, n=NFFT)\n",
    "\n",
    "            Y_obs_freq = np.fft.rfftfreq(NFFT, d=tr_obs_trim.stats.delta)\n",
    "            Y_syn_freq = np.fft.rfftfreq(NFFT, d=tr_syn_trim_shifted.stats.delta)\n",
    "            Y_obs_noise_freq = np.fft.rfftfreq(NFFT, d=tr_obs_noise.stats.delta)\n",
    "\n",
    "            # Apply smoothing\n",
    "            # ref: https://github.com/ROBelgium/MSNoise/blob/master/msnoise/s05compute_mwcs2.py#L92\n",
    "            Y_obs_smoothed = signal.convolve(Y_obs, hanningwindow.real, \"same\")\n",
    "            Y_syn_smoothed = signal.convolve(Y_syn, hanningwindow.real, \"same\")\n",
    "            Y_obs_noise_smoothed = signal.convolve(Y_obs_noise, hanningwindow.real, \"same\")\n",
    "\n",
    "\n",
    "            # compute B(omega)\n",
    "            B_attenu = Y_obs/Y_syn\n",
    "            Battenu_smoothed = Y_obs_smoothed/Y_syn_smoothed\n",
    "\n",
    "#             # Compute maximum cross-correlation function\n",
    "#             signal.correlate(tr_obs_trim.data, tr_syn_trim.data)\n",
    "\n",
    "            #--------------------------#\n",
    "            # plot figure for master plot\n",
    "            #--------------------------#\n",
    "            fig, [ax1, ax2] = plt.subplots(1, 2, figsize=(6., 2.87))#figsize=(9, 4.3))\n",
    "            \n",
    "            # plot P waveform\n",
    "            ax1.plot(tr_obs_trim.times()*1e6, tr_obs_trim.data*1e3, \"k-\", label=\"Observation\", lw=1.0, zorder=2)\n",
    "#             ax1.plot(tr_syn_trim.times()*1e6, tr_syn_trim.data*1e3, \"r-\", label=\"syn.\", lw=1.0)\n",
    "            \n",
    "            # plot the shfted trace\n",
    "#             ax1.plot(tr_syn_trim.times()*1e6, np.roll(tr_syn_trim.data, N_shift)*1e3, \"r-\", label=\"syn.\")\n",
    "            ax1.plot(tr_syn_trim_shifted.times()*1e6, tr_syn_trim_shifted.data*1e3, \"r-\", label=\"Model without attenuation\", zorder=3)\n",
    "            ax1.plot(tr_obs_noise.times()*1e6, tr_obs_noise.data*1e3, \"--\", c=\"gray\", label=\"Noise\", lw=1.0, zorder=1)\n",
    "\n",
    "            ax1.set_xlabel(\"Time [Î¼s]\")\n",
    "            ax1.set_ylabel(\"Velocity [mm/s]\")\n",
    "\n",
    "            # ax1.set_title(f\"AS{tr_obs.stats.station[2:]} {tr_obs.stats.dataindex[6:]} VR={VR:.2f} N_shift={N_shift}\")\n",
    "            bd_loc = int(tr_obs.stats.dataindex[6:].split(\"_\")[1])\n",
    "            ax1.set_title(f\"AS{tr_obs.stats.station[2:]} ball-drop at {bd_loc:.0f} mm VR={VR:.2f}\", fontsize=10.)\n",
    "#             ax2.set_xlim([0.0, 20])\n",
    "            \n",
    "            ylimit_wf_p = np.array([-Asyn, Asyn])*3e3\n",
    "\n",
    "            ax1.set_xlim([0.0, 15])\n",
    "        \n",
    "#             ax1.set_ylim(ylimit_wf_p)\n",
    "            ax1.set_ylim([-0.05, 0.05])\n",
    "    \n",
    "            # plot tukey taper window\n",
    "            taperwindow = signal.windows.hann(tr_obs_trim.stats.npts) # taper with 0.5 is equivalent to hanning window\n",
    "            taperwindow_scale = 0.04\n",
    "            ax1.plot(tr_obs_trim.times()*1e6, taperwindow*taperwindow_scale, \"g:\", label=\"\", lw=1)\n",
    "        \n",
    "            ax1.legend(loc=3, fontsize=8.6)\n",
    "            \n",
    "            # annotate info\n",
    "#             ax1.text(6.0, -0.11, f\"source distance = {tr_obs_trim.stats.dist:.1f}mm\\nincident angle    = {theta4:.0f}Â°\")\n",
    "#             ax1.text(5.5, -0.09, f\"source distance = {tr_obs_trim.stats.dist:.1f}mm\")\n",
    "        \n",
    "            # plot P wave spectra\n",
    "#             amp0 = np.max(np.abs(Y_obs)) # normalize the dB by the maximum of observation\n",
    "#             ax2.semilogx(Y_obs_freq/1e6,  20 * np.log10(np.abs(Y_obs)/amp0), \"k.--\", label=\"\", lw=1.0)\n",
    "#             ax2.semilogx(Y_syn_freq/1e6,  20 * np.log10(np.abs(Y_syn)/amp0), \"r.--\", label=\"\", lw=1.0)\n",
    "#             ax2.semilogx(Y_obs_freq/1e6,  20 * np.log10(np.abs(B_attenu)), \"g.--\", label=\"B(Ï‰)\")\n",
    "#             ax2.semilogx(Y_obs_noise_freq/1e6,  20 * np.log10(np.abs(Y_obs_noise)/amp0),  \".--\", c=\"gray\",label=\"\", lw=1.0)\n",
    "            \n",
    "#             amp0_smoothed = np.max(np.abs(Y_obs_smoothed))\n",
    "            # normalize the amplitude at 100kHz\n",
    "            amp0_freq = 100e3\n",
    "            print(f\"freq normalized at {amp0_freq}Hz\")\n",
    "            amp0_smoothed = np.interp(amp0_freq, Y_obs_freq, np.abs(Y_obs_smoothed))\n",
    "        \n",
    "            ax2.semilogx(Y_obs_freq/1e6,  20 * np.log10(np.abs(Y_obs_smoothed)/amp0_smoothed), \"k.-\", label=\"obs.\", lw=1.0, zorder=2)\n",
    "            ax2.semilogx(Y_syn_freq/1e6,  20 * np.log10(np.abs(Y_syn_smoothed)/amp0_smoothed), \"rd-\", label=\"syn.\", lw=1.0, ms=3.2, zorder=3)\n",
    "#             ax2.semilogx(Y_obs_freq/1e6,  20 * np.log10(np.abs(Battenu_smoothed)), \".-\", c=\"g\", label=\"\")\n",
    "            ax2.semilogx(Y_obs_noise_freq/1e6,  20 * np.log10(np.abs(Y_obs_noise_smoothed)/amp0_smoothed),  \"^--\", c=\"gray\", label=\"noise\", lw=1.0, ms=3, zorder=1)\n",
    "            \n",
    "            ax2.set_xlabel(\"Frequency [MHz]\")\n",
    "            ax2.set_ylabel(\"Amplitude [dB]\")\n",
    "\n",
    "            ax2.set_xlim([0.06, 2])\n",
    "            ax2.set_ylim([-60, 20])\n",
    "            ax2.grid(True, c=np.array([230, 230, 230])/255, lw=0.25, which=\"major\",)\n",
    "            ax2.set_axisbelow(True)\n",
    "            # ax2.legend(loc=1, fontsize=9)\n",
    "\n",
    "            ax2.xaxis.set_major_formatter(mticker.FormatStrFormatter('%.1f'))\n",
    "            # ax2.ticklabel_format(style='plain', axis='x')\n",
    "\n",
    "            # plot sensor natural frequency\n",
    "#             f_naturalfreq = 1e6 / 1e6\n",
    "#             ax2.axvline()\n",
    "            plt.subplots_adjust(wspace=1.0)\n",
    "\n",
    "            plt.tight_layout()\n",
    "#             plt.savefig(\"../figure/velocityspectra.png\", dpi=300)\n",
    "#             plt.savefig(\"../figure/velocityspectra.eps\")\n",
    "\n",
    "            plt.savefig(figdir_Bomega+\"/FigS_master_computeBomega_{}_window.png\".format(dataindex), format=\"png\", dpi=70, bbox_inches=\"tight\")\n",
    "            plt.savefig(figdir_Bomega+\"/FigS_master_computeBomega_{}_window.eps\".format(dataindex), format=\"eps\", bbox_inches=\"tight\")\n",
    "\n",
    "            # dump the data\n",
    "        \n",
    "            # Bomega_all[f\"{dataindex}_dist\"] = tr_obs.stats.dist\n",
    "            # Bomega_all[f\"{dataindex}_Y_obs\"] = Y_obs\n",
    "            # Bomega_all[f\"{dataindex}_Y_syn\"] = Y_syn\n",
    "            # Bomega_all[f\"{dataindex}_Y_obs_noise\"] = Y_obs_noise\n",
    "\n",
    "            # Bomega_all[f\"{dataindex}_Battenu\"] = B_attenu\n",
    "            # Bomega_all[f\"{dataindex}_Battenu_smoothed\"] = Battenu_smoothed\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the waveform with P window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ampfactor_comparison = np.zeros((0, 4))\n",
    "trcount = 0\n",
    "theta_evalerr = np.zeros(Nd)\n",
    "err_evalerr = np.zeros(Nd)\n",
    "\n",
    "vp = 6200\n",
    "vs = 3600\n",
    "\n",
    "hanningwindow = signal.windows.hann(2*smoothing_half_win+1).astype('complex')\n",
    "hanningwindow /= smoothing_half_win #len(hanningwindow) # divide by half win so that the summation of window weights is unity\n",
    "print(np.sum(hanningwindow))\n",
    "\n",
    "# smoothing half width in frequency\n",
    "\n",
    "dfreq= Y_obs_freq[1] - Y_obs_freq[0]\n",
    "smooth_half_win_freq = smoothing_half_win * dfreq\n",
    "print(f\"smoothing half win:{smooth_half_win_freq/1e3}kHz\")\n",
    "\n",
    "\n",
    "# for stnm in tqdm(AEsensors):\n",
    "for stnm in tqdm(AEsensors[19:20]):\n",
    "    # stnm = AEsensors[11]\n",
    "    finame = os.path.join(bdwaveform_datadir, \"{}_bdwaveform_longpretrig.pickle\".format(stnm)) \n",
    "    st = read(finame, format=\"PICKLE\")\n",
    "\n",
    "    # prefilitering traces\n",
    "    # demean\n",
    "#     st.detrend(type='demean')\n",
    "#     # apply bandpass\n",
    "#     st.filter(\"bandpass\", freqmin=freqmin, freqmax=freqmax, corners=4, zerophase=True)     # NO FILTER for the analysis of attenuation\n",
    "    # apply taper\n",
    "#     st.taper(max_percentage=0.001)   \n",
    "\n",
    "    # Select observation and synthetic\n",
    "    st_obs = st.select(channel='OY')\n",
    "    st_syn = st.select(channel='SY')\n",
    "    \n",
    "    # Extract the traces used in the Si inversion\n",
    "    for i, tr_obs in enumerate(st_obs):\n",
    "        if tr_obs.stats.dataindex not in np.squeeze(df_caseid.values):\n",
    "            print(tr_obs.stats.dataindex)\n",
    "            st_obs.remove(tr_obs)\n",
    "    \n",
    "    for i, tr_obs in enumerate(st_obs):\n",
    "            \n",
    "            if tr_obs.stats.location != selected_BDid:\n",
    "                continue\n",
    "            \n",
    "            dataindex = tr_obs.stats.dataindex\n",
    "            tr_syn = list(filter(lambda tr: tr.stats.dataindex == dataindex, st_syn))[0]\n",
    "\n",
    "            dist = tr_obs.stats.dist #[mm]\n",
    "\n",
    "            if dist <= dist_p_threshold:\n",
    "                p_theoretical = tr_obs.stats.tpick\n",
    "            else:\n",
    "                p_theoretical = dist/vmean\n",
    "\n",
    "            tvec = np.array(range(0, tr_syn.stats.npts))*tr_syn.stats.delta*1e3 - tr_obs.stats.pretrigger #[ms]\n",
    "\n",
    "            #---compute correction coefficient from Si, Tj and alpha(theta)---#\n",
    "            amp_correction = np.zeros(4) # 4 models\n",
    "            ii, jj = [cij[dataindex][0]-1, cij[dataindex][1]-1]\n",
    "            kk = df_caseid.index[df_caseid.caseid == dataindex][0]\n",
    "\n",
    "            Asyn = Aksyn[kk]\n",
    "\n",
    "            # model 1: gain model\n",
    "            Si1 = df_Si[\"model1\"][ii]\n",
    "            amp_correction[0] = Si1\n",
    "\n",
    "            # model 3: directionality-dominant\n",
    "            Si3 = df_Si[\"model3\"][ii]\n",
    "            theta3 = df_disttheta[\"theta\"][kk]\n",
    "#             alpha3 = model_bell(theta3, 1.0, a3, b3) # bell-shape \n",
    "#             amp_correction[1] = Si3*alpha3\n",
    "            TR3 = df_TR[\"model3\"][0]\n",
    "            k3 = incidentangle_scalingfactor_analytic(v, np.deg2rad(theta3), TR3, R)\n",
    "            amp_correction[1] = Si3*k3\n",
    "            #model 2: source impact-dominant\n",
    "            Si2 = df_Si[\"model2\"][ii]\n",
    "            Tj2 = df_Tj[\"model2\"][jj]\n",
    "            amp_correction[2] = Si2*Tj2\n",
    "            \n",
    "            # model 4: mix model\n",
    "            Si4 = df_Si[\"model4\"][ii]\n",
    "            Tj4 = df_Tj[\"model4\"][jj]\n",
    "            theta4 = df_disttheta[\"theta\"][kk]\n",
    "#             alpha4 = model_bell(theta4, 1.0, a4, b4) # bell-shape \n",
    "            TR4 = df_TR[\"model4\"][0]\n",
    "            k4 = incidentangle_scalingfactor_analytic(v, np.deg2rad(theta4), TR4, R)\n",
    "\n",
    "            amp_correction[3] = Si4*Tj4*k4\n",
    "\n",
    "#             print((Si4, Tj4, k4))\n",
    "            ampfactor_comparison = np.vstack((ampfactor_comparison, [df_disttheta[\"theta\"][kk], \n",
    "                            amp_correction[3]/amp_correction[0],  amp_correction[3]/amp_correction[1], amp_correction[3]/amp_correction[2]]))\n",
    "    \n",
    "            #---------------------------------------------------------------------------------#\n",
    "\n",
    "            # remove offset and detrend by the first 5 microsec\n",
    "            st0 = tr_obs.stats.starttime\n",
    "            pt = timedelta(milliseconds = tr_obs.stats.pretrigger)\n",
    "\n",
    "            dist = tr_obs.stats.dist #[mm]\n",
    "\n",
    "            if dist <= dist_p_threshold:\n",
    "                p_theoretical = tr_obs.stats.tpick\n",
    "            else:\n",
    "                p_theoretical = dist/vmean\n",
    "\n",
    "            # trim Pwave\n",
    "            starttime = st0+pt+(p_theoretical-pwin_len_pre)*1e-3 - pwin_len_margin\n",
    "            st_P = int(starttime.microsecond*1e-6/tr_obs.stats.delta)\n",
    "            noise_npts = int(5e-6*tr_obs.stats.sampling_rate)\n",
    "            for tr in [tr_obs, tr_syn]:\n",
    "                kp = np.polyfit(tr.times()[st_P:st_P+noise_npts], tr.data[st_P:st_P+noise_npts], 1)\n",
    "                tr.data -= np.polyval(kp, tr.times())\n",
    "\n",
    "            # Deprecated: Apply bandpass-filter to mitigate the numerical noise in synthetics\n",
    "            # just for the sake of ease in visuallization, we apply band-pass filter\n",
    "            freqmin = 0.06e6\n",
    "            freqmax = 1e6\n",
    "            sos = signal.butter(3, [freqmin, freqmax], 'bandpass', fs=tr_obs.stats.sampling_rate, output='sos')\n",
    "            tr_syn_filtered = tr_syn.copy()\n",
    "            tr_obs_filtered = tr_obs.copy()\n",
    "#             tr_syn_filtered.filter(\"bandpass\", freqmin=freqmin, freqmax=freqmax, corners=3, zerophase=False)    \n",
    "#             tr_obs_filtered.filter(\"bandpass\", freqmin=freqmin, freqmax=freqmax, corners=3, zerophase=False)    \n",
    "#             tr_syn_filtered.data = signal.sosfilt(sos, tr_syn_filtered.data)\n",
    "#             tr_obs_filtered.data = signal.sosfilt(sos, tr_obs_filtered.data)\n",
    "\n",
    "            fig, ax = plt.subplots(1, 1, figsize=(8, 2.8)) #2.8))\n",
    "            \n",
    "#             ax.plot(tvec, 1e3*tr_obs_filtered.data/amp_correction[-1], \"k-\", label=\"obs.\", lw=1.0)\n",
    "#             ax.plot(tvec, 1e3*tr_syn_filtered.data, 'r-', label=\"syn.\", lw=1.0)\n",
    "\n",
    "            # plot raw data\n",
    "            ax.plot(tvec, 1e3*tr_obs.data/amp_correction[-1], \"k-\", label=\"obs.\", lw=1.0)\n",
    "            ax.plot(tvec+N_shift*tr_syn.stats.delta*1e3, 1e3*tr_syn.data, 'r-', label=\"syn.\", lw=1.0)\n",
    "          \n",
    "            ax.set_xlabel(\"Time [ms]\")\n",
    "            ax.set_ylabel(\"Velocity [mm/s]\")\n",
    "            \n",
    "            ax.set_xlim([0, 0.08]) #[0.035, 0.05])\n",
    "#             ax.set_xlim([0.04, 0.06])\n",
    "            \n",
    "#             ax1.set_ylim(ylimit_wf_p)\n",
    "            ylimit = [-0.25, 0.25]\n",
    "            ax.set_ylim(ylimit)\n",
    "#             ax.set_ylim([-0.001, 0.001])\n",
    "    \n",
    "            #----------------------------------#\n",
    "            # annotate arrival times and window\n",
    "            #----------------------------------#\n",
    "\n",
    "            dist = tr_obs_filtered.stats.dist\n",
    "            tp = dist/vp\n",
    "            ts = dist/vs\n",
    "            if dist <= dist_p_threshold:\n",
    "                p_theoretical = tr_obs.stats.tpick\n",
    "            else:\n",
    "                p_theoretical = dist/vmean\n",
    "                \n",
    "#             ax.plot(tr_obs_trim_PLT.stats.dist/param[\"cp\"], arrivaltime_y-i, \"v\", ms=5.5, mec=\"k\", mfc=\"w\")\n",
    "#     ax.plot(tr_obs_trim_PLT.stats.dist/param[\"cs\"], arrivaltime_y-i, \"s\", ms=5, mec=\"k\", mfc=\"w\")\n",
    "#             ax.plot(tp, 0.3, \"v\", ms=5.5, mec=\"k\", mfc=\"w\")\n",
    "            ax.plot(ts, 0.1, \"s\", ms=5, mec=\"k\", mfc=\"w\")\n",
    "#             ax.text(tp, 0.35, \"P\", ha=\"center\")\n",
    "            ax.text(ts, 0.125, \"S\", ha=\"center\")\n",
    "            \n",
    "            # trimmed windows\n",
    "            \n",
    "            st0 = tr_obs.stats.starttime\n",
    "            pt = timedelta(milliseconds = tr_obs.stats.pretrigger)\n",
    "            \n",
    "            centralize_tshift = Ncentralize * tr_obs.stats.delta # observed and synthetic waveforms are shifted to centralize the first peak\n",
    "            p_starttime = (p_theoretical-pwin_len_pre)*1e-3 + centralize_tshift\n",
    "            p_endtime = (p_theoretical-pwin_len_pre+pwin_len)*1e-3 + centralize_tshift\n",
    "            \n",
    "            bar_y = 0.06\n",
    "            ax.plot(np.array([p_starttime, p_endtime])*1e3, np.array([bar_y, bar_y]), \"k-\", marker=\"|\" )\n",
    "            ax.text((p_starttime+p_endtime)*1e3/2, 0.09, \"P\", ha=\"center\")\n",
    "#             ax.plot((p_starttime+p_endtime)*1e3/2, 0.175, \"o\")\n",
    "#             ax.axhline(0.04)\n",
    "\n",
    "            # trim synthetic data\n",
    "            tr_syn_trim = tr_syn.copy() # avoid error in multiple trim\n",
    "            tr_syn_trim.trim(starttime, endtime, pad=True, fill_value=0.0)\n",
    "\n",
    "            # trim the observation noise\n",
    "            noise_start_time = p_starttime - noise_winlen\n",
    "            noise_end_time = p_theoretical*1e-3-noise_Pbackward\n",
    "            \n",
    "            bar_y_noise = -0.05\n",
    "            ax.plot(np.array([noise_start_time, noise_end_time])*1e3, np.array([bar_y_noise, bar_y_noise]), \"k-\", marker=\"|\" )\n",
    "            ax.text((noise_start_time+noise_end_time)*1e3/2, -0.11, \"background noise\", ha=\"center\")\n",
    "            \n",
    "            ax.set_title(f\"AS{tr_obs.stats.station[2:]} {tr_obs.stats.dataindex[6:]}\")\n",
    "            \n",
    "            ax.legend(loc=2)\n",
    "            ax.text(0.0025, -0.2, f\"source distance = {tr_obs_trim.stats.dist:.1f}mm\")\n",
    "\n",
    "            \n",
    "            plt.tight_layout()\n",
    "\n",
    "\n",
    "            plt.savefig(figdir_Bomega+\"/trace_master_computeBomega_{}.png\".format(dataindex), format=\"png\", dpi=150, bbox_inches=\"tight\")\n",
    "            plt.savefig(figdir_Bomega+\"/trace_master_computeBomega_{}.eps\".format(dataindex), format=\"eps\", bbox_inches=\"tight\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
